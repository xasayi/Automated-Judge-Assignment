{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbfbde4",
   "metadata": {},
   "source": [
    "# 0. Setup Data Folder and Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# input the folder name, judge folder, and venture folder\n",
    "# you can navigate to these folders to see what they contain\n",
    "folder_name = \"example\" # CHANGE THIS\n",
    "judge_folder = f\"{folder_name}/Input/Judge_Folder\" # CHANGE THIS\n",
    "venture_folder = f\"{folder_name}/Input/Venture_Folder\" # CHANGE THIS\n",
    "\n",
    "# create output folder  \n",
    "os.makedirs(f\"{folder_name}/Output\", exist_ok=True)\n",
    "\n",
    "# HELPER FUNCTION to print the max simliarity pair\n",
    "def print_max_pair(matrix: np.ndarray, ind_to_judge: dict, ind_to_venture: dict) -> None:\n",
    "    '''\n",
    "    args:\n",
    "        matrix: similarity matrix for judge and ventures\n",
    "        ind_to_judge: mapping from judge index to judge code\n",
    "        ind_to_venture: mapping from venture index to venture code \n",
    "    '''\n",
    "    # gets the judge and venture index of the max similarity pair\n",
    "    judge_i, venture_i = np.unravel_index(np.argmax(matrix), matrix.shape)\n",
    "    max_sim_score = np.max(matrix)\n",
    "    # print the judge and venture descriptions and simlilarity score\n",
    "    print('Similarity score:', max_sim_score)\n",
    "    with open(f'{folder_name}/Input/Judge_Folder/{ind_to_judge[judge_i]}.txt', 'r') as f:\n",
    "        print(f'Judge {ind_to_judge[judge_i]}.txt, {judge_i}')\n",
    "        print(f.read())\n",
    "    with open(f'{folder_name}/Input/Venture_Folder/{ind_to_venture[venture_i]}.txt', 'r') as f:\n",
    "        print(f'Venture {ind_to_venture[venture_i]}.txt, {venture_i}')\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175d14f",
   "metadata": {},
   "source": [
    "# 1. Compute Similarity Matrices from Base Learners\n",
    "\n",
    "In this section, we find the similarity matrices from the base learners. We have three types of base learners:\n",
    "\n",
    "* TF-IDF based.\n",
    "    * Augmented TF-IDF.\n",
    "    * TF-IDF with IDF smoothing.\n",
    "* Transformer-embedding based.\n",
    "    * Token-level.\n",
    "    * Document-level.\n",
    "* Hybrid learners that combine TF-IDF with transforemr-based embeddings.\n",
    "    * Token-level.\n",
    "    * Document-level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e4c34",
   "metadata": {},
   "source": [
    "## TF-IDF Base Learners\n",
    "### A. Augmented TF-IDF\n",
    " \n",
    "Modified from https://github.com/niharshah/TFIDFsimilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40aa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.tfidf_sim import compute_tfidf_sim_aug\n",
    "\n",
    "# get the similarity matrix and the judge and venture mappings from their name to index\n",
    "similarity_matrix_tfidf_aug, ind_to_judge, ind_to_venture = compute_tfidf_sim_aug(\n",
    "                                                                judge_folder=judge_folder, # judge folder name\n",
    "                                                                venture_folder=venture_folder, # venture folder name\n",
    "                                                                wiki_folder=None, # input None or the wikipedia folder to supplement the idf\n",
    "                                                                stem=True, # whether we stem when preprocessing the data\n",
    "                                                                sanitize=True, # whether we sanitize when preprocessing the data\n",
    "                                                                keep_ui=False) # whether to keep the uninformative words or not when preprocessing the data\n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/tfidf_aug_similarities.txt\", similarity_matrix_tfidf_aug)\n",
    "# we can print the judge and venture that have the highest similarity from this similarity matrix\n",
    "print_max_pair(similarity_matrix_tfidf_aug, ind_to_judge, ind_to_venture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ffb2a",
   "metadata": {},
   "source": [
    "### B. TF-IDF with IDF smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77814018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.tfidf_sim import compute_tfidf_sim\n",
    "\n",
    "# get the similarity matrix\n",
    "similarity_matrix_tfidf, _, _ = compute_tfidf_sim(\n",
    "                                                                judge_folder=judge_folder, # judge folder name\n",
    "                                                                venture_folder=venture_folder, # venture folder name\n",
    "                                                                wiki_folder=None, # input None or the wikipedia folder to supplement the idf\n",
    "                                                                stem=True, # whether we stem when preprocessing the data\n",
    "                                                                sanitize=True, # whether we sanitize when preprocessing the data\n",
    "                                                                keep_ui=False) # whether to keep the uninformative words or not when preprocessing the data\n",
    "\n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/tfidf_similarities.txt\", similarity_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5497af",
   "metadata": {},
   "source": [
    "## Transformer-based Embedding Base Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad2953",
   "metadata": {},
   "source": [
    "### A. Token-level\n",
    "\n",
    "For a venture and a judge, token-level finds the cosine similarity between individual token embeddings from the judge and the venture, and takes the mean over all the token-level similarities as the final similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eeb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.embed_sim import compute_embedding_sim\n",
    "\n",
    "# get the token level similarity matrix\n",
    "similarity_matrix_embed_token, _, _ = compute_embedding_sim(judge_folder=judge_folder, # judge folder name\n",
    "                                                                venture_folder=venture_folder, # venture folder name\n",
    "                                                                model='bert-base-uncased', # the name of the model used from huggingface\n",
    "                                                                stem=True, # whether we stem when preprocessing the data\n",
    "                                                                sanitize=True, # whether we sanitize when preprocessing the data\n",
    "                                                                keep_ui=True, # whether to keep the uninformative words or not when preprocessing the data\n",
    "                                                                token_level=True) # whether to use token level similarity or not \n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/embed_token_similarities.txt\", similarity_matrix_embed_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4762d",
   "metadata": {},
   "source": [
    "### B. Document-level\n",
    "\n",
    "For a venture and a judge, document-level takes the mean token embeddings for the venture and the judge embedding and finds the cosine similarity between the mean venture embedding and the mean judge embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the document level similarity matrix\n",
    "similarity_matrix_embed, _, _ = compute_embedding_sim(judge_folder=judge_folder, \n",
    "                                                                venture_folder=venture_folder, \n",
    "                                                                model='bert-base-uncased',\n",
    "                                                                stem=True, \n",
    "                                                                sanitize=True, \n",
    "                                                                keep_ui=True,\n",
    "                                                                token_level=False)\n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/embed_similarities.txt\", similarity_matrix_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0c20e",
   "metadata": {},
   "source": [
    "## Hybrid Base Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc680778",
   "metadata": {},
   "source": [
    "Get the embeddings, weights, attention masks, and judge/venture mappings necessary to compute the hybrid model similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.hybrid_sim import get_hybrid_embeddings\n",
    "# get the embeddings, weights, masks the judge and ventures\n",
    "judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, _, _ = get_hybrid_embeddings(\n",
    "                                                                            judge_folder=judge_folder, # judge folder name\n",
    "                                                                            venture_folder=venture_folder, # venture folder name\n",
    "                                                                            wiki_folder=None, # input None or the wikipedia folder to supplement the idf\n",
    "                                                                            model='bert-base-uncased', # the name of the model used \n",
    "                                                                            augmented_idf=False, # whether to use the augmented idf or use the vanilla idf\n",
    "                                                                            stem=True, # whether we stem when preprocessing the data\n",
    "                                                                            sanitize=True, # whether we sanitize when preprocessing the data\n",
    "                                                                            keep_ui=True) # whether to keep the uninformative words or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec742255",
   "metadata": {},
   "source": [
    "### A. Token-level\n",
    "\n",
    "For a venture and a judge, we weigh the token-level simialrity between a token embedding from the judge and a token embedding from the venture by their corresponding IDF weights. Then we take the average across all weighted token similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1435751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.hybrid_sim import compute_hybrid_sim\n",
    "# get the hybrid similarity matrix\n",
    "similarity_matrix_hybrid_token = compute_hybrid_sim(\n",
    "                                                    judge_embeddings, \n",
    "                                                    judge_idf_weights, \n",
    "                                                    judge_mask, \n",
    "                                                    venture_embeddings, \n",
    "                                                    venture_idf_weights, \n",
    "                                                    venture_mask, \n",
    "                                                    token_level=True)\n",
    "\n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/hybrid_token_similarities.txt\", similarity_matrix_hybrid_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572750d4",
   "metadata": {},
   "source": [
    "### B. Document-level\n",
    "For a venture and a judge, we take a weighted average of the token embeddings for the judge or venture embedding. Then we calculate the cosine similarity between the weighted token embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.hybrid_sim import compute_hybrid_sim\n",
    "# get the hybrid similarity matrix\n",
    "similarity_matrix_hybrid = compute_hybrid_sim(\n",
    "                                                    judge_embeddings, \n",
    "                                                    judge_idf_weights, \n",
    "                                                    judge_mask, \n",
    "                                                    venture_embeddings, \n",
    "                                                    venture_idf_weights, \n",
    "                                                    venture_mask, \n",
    "                                                    token_level=False)\n",
    "\n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/hybrid_similarities.txt\", similarity_matrix_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428e72d",
   "metadata": {},
   "source": [
    "# 2. Ensemble Learning\n",
    "\n",
    "Now that we have the base learner similarity matrices, we can combine them into an ensemble learner through linear regression. First, we generate a random match quality dataset as the ground truth dataset. In actual evaluations, these match quality scores would be curated by human experts, assigning a score of 1 to 5 for each judge-venture pair, where 5 indicates an excellent match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef039bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "folder_name = \"example\"\n",
    "\n",
    "def get_random_manual_scores(folder_name: str) -> dict:\n",
    "    '''\n",
    "    args:\n",
    "        folder_name: name of the data folder\n",
    "    return:\n",
    "        scores: a mapping of the judge-venture pairs to their randomly generated match quality score\n",
    "    '''\n",
    "    scores = {}\n",
    "    judge_folder = f\"{folder_name}/Input/Judge_Folder\"\n",
    "    venture_folder = f\"{folder_name}/Input/Venture_Folder\"\n",
    "\n",
    "    j_folder_path = Path(judge_folder)\n",
    "    v_folder_path = Path(venture_folder)\n",
    "    n_judges = len([i for i in j_folder_path.glob(\"*.txt\")])\n",
    "    n_ventures = len([i for i in v_folder_path.glob(\"*.txt\")])\n",
    "    for i in range(n_judges):\n",
    "        for j in range(n_ventures):\n",
    "            scores[(i, j)] = np.random.randint(1,5)\n",
    "    return scores\n",
    "\n",
    "manual_scores = get_random_manual_scores(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9c0a0",
   "metadata": {},
   "source": [
    "We use a linear regression with convex constraints to combine the similarity matrices from the base learners into one ensemble learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble import optimize_similarity, get_score_difference\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "filenames = [f'{folder_name}/Output/embed_similarities.txt', \n",
    "             f'{folder_name}/Output/hybrid_similarities.txt', \n",
    "             f'{folder_name}/Output/tfidf_similarities.txt']\n",
    "# get a random set of manual scores\n",
    "manual_scores = get_random_manual_scores(folder_name)\n",
    "# define k fold \n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "avg = []\n",
    "\n",
    "score_keys = list(manual_scores.keys())\n",
    "score_vals = list(manual_scores.values())\n",
    "all_weights = []\n",
    "# iterate through each fold\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(score_keys)):\n",
    "    print(\"\\nCross-validation\", i)\n",
    "    # get the train data and test data \n",
    "    train_score_dic = {\n",
    "        score_keys[j]: score_vals[j] for j in train_index\n",
    "    }\n",
    "    test_score_dic = {\n",
    "        score_keys[j]: score_vals[j] for j in test_index\n",
    "    }\n",
    "\n",
    "    # optimize over the algorithmic methods\n",
    "    weights, X_train, y_train, train_mean_diff = optimize_similarity(filenames, train_score_dic)\n",
    "    # get the test loss\n",
    "    test_mean_diff = get_score_difference(filenames, test_score_dic, weights)\n",
    "    all_weights.append(weights)\n",
    "    print(\"Average Train Difference:\", train_mean_diff)\n",
    "    print('Average Test Difference:', test_mean_diff)\n",
    "\n",
    "# get the average of weights from the 5 fold \n",
    "all_weights = np.array(all_weights)\n",
    "avg_weights = np.mean(all_weights, axis=0)\n",
    "diff = get_score_difference(filenames, manual_scores, avg_weights)\n",
    "print(\"\\nFinal Avg Weight Diff:\", diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3bf14",
   "metadata": {},
   "source": [
    "# 3. Kendall Tau-b Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a624f",
   "metadata": {},
   "source": [
    "We use the Kendall tau-b rank correlation to evaluate the predicted similarity scores against the ground-truth scores. This metric enables us to assess how well different models perform relative to each other. (In this tutorial, since we use a randomly generated dataset of match quality scores, the results do not reflect real-world performance. In practice, one would use match quality scores curated by human experts for a valid evaluation.)\n",
    "* If $\\tau=+1$, there's perfect positive monotonic association (as one variable's rank increases, the other's consistently increases).\n",
    "* If $\\tau=-1$, there's perfect negative monotonic association (as one variable's rank increases, the other's consistently decreases).\n",
    "* If $\\tau=0$, there's no monotonic association between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed8a3a",
   "metadata": {},
   "source": [
    "We can find the kendall tau-b for an single base learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import get_percentile, get_ranking\n",
    "# load the similarity matrix\n",
    "sim_mat = np.loadtxt(f\"{folder_name}/Output/embed_similarities.txt\")\n",
    "# define output path\n",
    "output_path = f\"{folder_name}/Output/embed_kendall_tau.txt\"\n",
    "# get the percentile distribution of scores of the manual score\n",
    "percentiles = get_percentile(manual_scores)\n",
    "# get the kendall tau-b rank coefficients between the manual scores and algorithmic scores and the detailed \n",
    "score_ranks = get_ranking(sim_mat, manual_scores, percentiles, output_path)\n",
    "with open(output_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b3390d",
   "metadata": {},
   "source": [
    "We can find the kendall tau-b for the ensemble learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble import get_input_and_labels\n",
    "avg_weights = np.mean(all_weights, axis=0)\n",
    "filenames = [f'{folder_name}/Output/embed_similarities.txt', \n",
    "             f'{folder_name}/Output/hybrid_similarities.txt', \n",
    "             f'{folder_name}/Output/tfidf_similarities.txt']\n",
    "\n",
    "X, _, _ = get_input_and_labels(filenames, manual_scores)\n",
    "ensemble_similarity = np.dot(weights, X.T)\n",
    "\n",
    "output_path = f\"{folder_name}/Output/ensemble_kendall_tau.txt\"\n",
    "# get the percentile distribution of scores of the manual score\n",
    "percentiles = get_percentile(manual_scores)\n",
    "# get the kendall tau-b rank coefficients between the manual scores and algorithmic scores\n",
    "score_ranks = get_ranking(ensemble_similarity, manual_scores, percentiles, output_path)\n",
    "with open(output_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
