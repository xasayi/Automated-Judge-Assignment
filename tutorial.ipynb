{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbfbde4",
   "metadata": {},
   "source": [
    "#  1. Get similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d64cf",
   "metadata": {},
   "source": [
    "run the following cell to specify what data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# input the data folder structured as the following\n",
    "folder_name = \"example\"\n",
    "judge_folder = f\"{folder_name}/Input/Judge_Folder\"\n",
    "venture_folder = f\"{folder_name}/Input/Venture_Folder\"\n",
    "\n",
    "# create output folder  \n",
    "os.makedirs(f\"{folder_name}/Output\", exist_ok=True)\n",
    "\n",
    "def print_max_pair(matrix: np.ndarray, judge_to_ind: dict, venture_to_ind: dict):\n",
    "    '''prints the matched judge-venture pair that has the highest similarity score'''\n",
    "    judge_i, venture_i = np.unravel_index(np.argmax(matrix), matrix.shape)\n",
    "    max_sim_score = np.max(matrix)\n",
    "    ind_to_judge = {ind:judge for judge, ind in judge_to_ind.items()}\n",
    "    ind_to_venture = {ind:venture for venture, ind in venture_to_ind.items()}\n",
    "    print('Similarity score:', max_sim_score)\n",
    "    with open(f'{folder_name}/Input/Judge_Folder/{ind_to_judge[judge_i]}.txt', 'r') as f:\n",
    "        print(f'Judge {ind_to_judge[judge_i]}.txt, {judge_i}')\n",
    "        print(f.read())\n",
    "    with open(f'{folder_name}/Input/Venture_Folder/{ind_to_venture[venture_i]}.txt', 'r') as f:\n",
    "        print(f'Venture {ind_to_venture[venture_i]}.txt, {venture_i}')\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e4c34",
   "metadata": {},
   "source": [
    "## Using TFIDF\n",
    "### 1. Use augmented TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40aa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.tfidf_sim import get_aug_tfidf, tfidf_sim_aug\n",
    "from preprocess_data import get_parsed_data\n",
    "\n",
    "def get_tfidf_sim_aug(judge_folder, venture_folder, wiki_folder, stem, sanitize, lines, keep_ui):\n",
    "    '''get the similarity matric using augmented tfidf'''\n",
    "    judges, ventures = get_parsed_data(judge_folder, venture_folder, stem, sanitize, lines, keep_ui)\n",
    "    judge_to_ind = {judge:ind for ind, judge in enumerate(judges.keys())}\n",
    "    venture_to_ind = {venture:ind for ind, venture in enumerate(ventures.keys())}\n",
    "    idf, _, = get_aug_tfidf([judges, ventures], wiki_folder, stem, sanitize, lines, keep_ui)\n",
    "    similarity_matrix = -np.ones((len(judges), len(ventures)))\n",
    "\n",
    "    for judge, j_ind in judge_to_ind.items():\n",
    "        for venture, v_ind in venture_to_ind.items():\n",
    "            j_counter, v_counter = judges[judge], ventures[str(venture)]\n",
    "\n",
    "            if len(j_counter) != 0 and len(v_counter) != 0:\n",
    "                similarity_matrix[j_ind, v_ind] = tfidf_sim_aug(\n",
    "                    j_counter, v_counter, idf\n",
    "                )\n",
    "    return similarity_matrix, judge_to_ind, venture_to_ind\n",
    "\n",
    "# get the similarity matrix and the judge and venture mappings\n",
    "similarity_matrix_tfidf_aug, judge_to_ind, venture_to_ind = get_tfidf_sim_aug(judge_folder=judge_folder, \n",
    "                                                                venture_folder=venture_folder, \n",
    "                                                                wiki_folder=None, # input None or the wikipedia folder to supplement the idf\n",
    "                                                                stem=True, # whether we stem when preprocessing the data\n",
    "                                                                sanitize=True, # whether we sanitize when preprocessing the data\n",
    "                                                                lines=False, # whether we want to input a str passage (lines=True) or a list of str words (lines=False)\n",
    "                                                                keep_ui=False) # whether to keep the uninformative words or not\n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/tfidf_aug_similarities.txt\", similarity_matrix_tfidf_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eceae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.08026348218648928\n",
      "Judge 2.txt, 4\n",
      "Dr. Kenji serves as CTO-in-Residence at FutureHealth Capital, where he advises portfolio companies at the intersection of life sciences and machine learning. A former practicing physician and biomedical researcher, Kenji has founded and exited two medtech startups, bringing a wealth of experience in product development, FDA navigation, and scientific validation. In judging settings, he is both compassionate and deeply analytical, often challenging founders on their clinical claims, data robustness, and regulatory readiness.\n",
      "Venture 4.txt, 1\n",
      "Neuron is a neurotechnology startup founded in 2025 and based in Toronto. The company is developing a wearable EEG headband that integrates with a personalized neuro-coaching platform, helping users improve focus, sleep, and cognitive performance. Backed by $2.4 million in early-stage funding, Neuron combines machine learning with neural signal processing, positioning itself as a category-defining product in the consumer neurotech space. With pilots underway at two research hospitals and early traction in the quantified-self community, theyâ€™re now seeking $4 million in Series A funding to pursue FDA Class II designation and expand their data pipeline.\n"
     ]
    }
   ],
   "source": [
    "print_max_pair(similarity_matrix_tfidf_aug, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1a541",
   "metadata": {},
   "source": [
    "### 2. Use vanilla TFIDF with IDF smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77814018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from preprocess_data import parse_info, get_parsed_data\n",
    "\n",
    "wiki_folder = None\n",
    "stem = True\n",
    "sanitize = True\n",
    "lines = True\n",
    "keep_ui = False\n",
    "\n",
    "def get_tfidf_sim(judge_folder, venture_folder, wiki_folder, stem, sanitize, lines, keep_ui):\n",
    "    '''get the similarity matric using vanilla tfidf with idf smoothing'''\n",
    "    count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    tfidf = TfidfTransformer(\n",
    "        smooth_idf=True, use_idf=True, norm=\"l2\", sublinear_tf=False\n",
    "    )\n",
    "\n",
    "    judges, ventures = get_parsed_data(judge_folder, venture_folder, stem, sanitize, lines, keep_ui)\n",
    "    judge_to_ind = {judge:ind for ind, judge in enumerate(judges.keys())}\n",
    "    venture_to_ind = {venture:ind for ind, venture in enumerate(ventures.keys())}\n",
    "\n",
    "    judge_values = list(judges.values())\n",
    "    venture_values = list(ventures.values())\n",
    "\n",
    "    if wiki_folder is not None:\n",
    "        wiki_dic = parse_info(wiki_folder, stem, sanitize, lines, keep_ui)\n",
    "        wiki_values = list(wiki_dic.values())\n",
    "        word_count_vec = count_vectorizer.fit_transform(\n",
    "            judge_values + venture_values + wiki_values\n",
    "        )\n",
    "    else:\n",
    "        word_count_vec = count_vectorizer.fit_transform(judge_values + venture_values)\n",
    "\n",
    "    tfidf.fit(word_count_vec)\n",
    "    count_vec = count_vectorizer.transform(judge_values + venture_values)\n",
    "    tfidf_vec = tfidf.transform(count_vec)\n",
    "    similarity_matrix = linear_kernel(tfidf_vec, tfidf_vec)[\n",
    "        : len(judge_values), len(judge_values) :\n",
    "    ]\n",
    "    return similarity_matrix, judge_to_ind, venture_to_ind \n",
    "\n",
    "# get the similarity matrix and the judge and venture mappings\n",
    "similarity_matrix_tfidf, judge_to_ind, venture_to_ind = get_tfidf_sim(judge_folder=judge_folder, \n",
    "                                                                venture_folder=venture_folder, \n",
    "                                                                wiki_folder=None, # input None or the wikipedia folder to supplement the idf\n",
    "                                                                stem=True, # whether we stem when preprocessing the data\n",
    "                                                                sanitize=True, # whether we sanitize when preprocessing the data\n",
    "                                                                lines=True, # whether we want to input a str passage (lines=True) or a list of str words (lines=False)\n",
    "                                                                keep_ui=False) # whether to keep the uninformative words or not\n",
    "\n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/tfidf_similarities.txt\", similarity_matrix_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f220ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.1067768235868722\n",
      "Judge 2.txt, 4\n",
      "Dr. Kenji serves as CTO-in-Residence at FutureHealth Capital, where he advises portfolio companies at the intersection of life sciences and machine learning. A former practicing physician and biomedical researcher, Kenji has founded and exited two medtech startups, bringing a wealth of experience in product development, FDA navigation, and scientific validation. In judging settings, he is both compassionate and deeply analytical, often challenging founders on their clinical claims, data robustness, and regulatory readiness.\n",
      "Venture 4.txt, 1\n",
      "Neuron is a neurotechnology startup founded in 2025 and based in Toronto. The company is developing a wearable EEG headband that integrates with a personalized neuro-coaching platform, helping users improve focus, sleep, and cognitive performance. Backed by $2.4 million in early-stage funding, Neuron combines machine learning with neural signal processing, positioning itself as a category-defining product in the consumer neurotech space. With pilots underway at two research hospitals and early traction in the quantified-self community, theyâ€™re now seeking $4 million in Series A funding to pursue FDA Class II designation and expand their data pipeline.\n"
     ]
    }
   ],
   "source": [
    "print_max_pair(similarity_matrix_tfidf, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5497af",
   "metadata": {},
   "source": [
    "## Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eeb745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarinaxi/anaconda3/envs/hbs/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took 0.34 seconds\n",
      "Tokenization took 0.27 seconds\n",
      "Calculating similarity matrix took: 0.01 seconds\n",
      "Tokenization took 0.44 seconds\n",
      "Tokenization took 0.61 seconds\n",
      "Calculating similarity matrix took: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from similarity.embed_sim import get_tokenizer_and_model, tokenize, token_similarity\n",
    "from preprocess_data import get_parsed_data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def get_embedding_sim(judge_folder, venture_folder, model, stem, sanitize, lines, keep_ui, token_level):\n",
    "    '''get the similarity matric using embeddings'''\n",
    "    judges, ventures = get_parsed_data(judge_folder, venture_folder, stem, sanitize, lines, keep_ui)\n",
    "    judge_to_ind = {judge:ind for ind, judge in enumerate(judges.keys())}\n",
    "    venture_to_ind = {venture:ind for ind, venture in enumerate(ventures.keys())}\n",
    "\n",
    "    judge_rows = [(judge_to_ind[judge], judges[judge]) for judge in judge_to_ind]\n",
    "    judge_df = pd.DataFrame(judge_rows, columns=[\"index\", \"info\"])\n",
    "    venture_rows = [(venture_to_ind[venture], ventures[venture]) for venture in venture_to_ind]\n",
    "    venture_df = pd.DataFrame(venture_rows, columns=[\"index\", \"info\"])\n",
    "\n",
    "    tokenizer, pretrained_model = get_tokenizer_and_model(model)\n",
    "    judge_embeddings, mean_judge_embed, judge_mask = tokenize(judge_df, model, tokenizer, pretrained_model)\n",
    "    venture_embeddings, mean_venture_embed, venture_mask = tokenize(venture_df, model, tokenizer, pretrained_model)\n",
    "    start = time.time()\n",
    "    if not token_level:\n",
    "        similarity_matrix = cosine_similarity(mean_judge_embed.cpu().numpy(), mean_venture_embed.cpu().numpy())\n",
    "    else:\n",
    "        similarity_matrix = token_similarity(judge_embeddings, judge_mask, venture_embeddings, venture_mask)\n",
    "    end = time.time()\n",
    "    print(f'Calculating similarity matrix took: {round(end-start, 2)} seconds')\n",
    "    return similarity_matrix, judge_to_ind, venture_to_ind\n",
    "\n",
    "# get the similarity matrix and the judge and venture mappings\n",
    "similarity_matrix_embed_token, judge_to_ind, venture_to_ind = get_embedding_sim(judge_folder=judge_folder, \n",
    "                                                                venture_folder=venture_folder, \n",
    "                                                                model='allenai/scibert_scivocab_uncased', # the name of the model used \n",
    "                                                                stem=True, # whether we stem when preprocessing the data\n",
    "                                                                sanitize=True, # whether we sanitize when preprocessing the data\n",
    "                                                                lines=True, # whether we want to input a str passage (lines=True) or a list of str words (lines=False)\n",
    "                                                                keep_ui=True, # whether to keep the uninformative words or not\n",
    "                                                                token_level=True) # whether to use token level similarity or not \n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/embed_token_similarities.txt\", similarity_matrix_embed_token)\n",
    "# get the similarity matrix and the judge and venture mappings\n",
    "similarity_matrix_embed, judge_to_ind, venture_to_ind = get_embedding_sim(judge_folder=judge_folder, \n",
    "                                                                venture_folder=venture_folder, \n",
    "                                                                model='allenai/scibert_scivocab_uncased',\n",
    "                                                                stem=True, \n",
    "                                                                sanitize=True, \n",
    "                                                                lines=True, \n",
    "                                                                keep_ui=True,\n",
    "                                                                token_level=False)\n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/embed_similarities.txt\", similarity_matrix_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc3fab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.9153174\n",
      "Judge 1.txt, 5\n",
      "E.T is a General Partner at Meridian Ventures, where she leads investments in climate tech and industrial innovation. Prior to her transition into venture capital, Elena spent a decade as a VP at Amazon Logistics, giving her a deep understanding of supply chains and operational scaling. She is known for her sharp diligence and no-nonsense feedback, often honing in on unit economics, go-to-market assumptions, and the scalability of hardware solutions. Her portfolio includes companies like Amply Power, Rivertown Robotics, and TerraMat.\n",
      "Venture 4.txt, 1\n",
      "Neuron is a neurotechnology startup founded in 2025 and based in Toronto. The company is developing a wearable EEG headband that integrates with a personalized neuro-coaching platform, helping users improve focus, sleep, and cognitive performance. Backed by $2.4 million in early-stage funding, Neuron combines machine learning with neural signal processing, positioning itself as a category-defining product in the consumer neurotech space. With pilots underway at two research hospitals and early traction in the quantified-self community, theyâ€™re now seeking $4 million in Series A funding to pursue FDA Class II designation and expand their data pipeline.\n"
     ]
    }
   ],
   "source": [
    "print_max_pair(np.array(similarity_matrix_embed), judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af28a7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.5268329977989197\n",
      "Judge 4.txt, 1\n",
      "Dr. Priya is a neuroscientist turned venture capitalist, currently a Partner at Synthesis Capital where she leads investments in neurotech, brain-computer interfaces, and digital mental health. A former professor at Oxford and author of multiple papers on real-time EEG signal decoding, she brings scientific rigor and high expectations to any founder pitching in the brain-health space. Sheâ€™s particularly drawn to ventures blend deep science with consumer application, but is known for grilling teams hard on clinical validity and IP defensibility.\n",
      "Venture 4.txt, 1\n",
      "Neuron is a neurotechnology startup founded in 2025 and based in Toronto. The company is developing a wearable EEG headband that integrates with a personalized neuro-coaching platform, helping users improve focus, sleep, and cognitive performance. Backed by $2.4 million in early-stage funding, Neuron combines machine learning with neural signal processing, positioning itself as a category-defining product in the consumer neurotech space. With pilots underway at two research hospitals and early traction in the quantified-self community, theyâ€™re now seeking $4 million in Series A funding to pursue FDA Class II designation and expand their data pipeline.\n"
     ]
    }
   ],
   "source": [
    "print_max_pair(np.array(similarity_matrix_embed_token), judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0c20e",
   "metadata": {},
   "source": [
    "## Using Hybrid Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from similarity.tfidf_sim import get_aug_tfidf, get_smoothed_tfidf\n",
    "from similarity.hybrid_sim import tokenize_hybrid, get_hybrid_sim\n",
    "from similarity.embed_sim import get_tokenizer_and_model\n",
    "from preprocess_data import get_parsed_data\n",
    "\n",
    "\n",
    "def get_hybrid_embeddings(judge_folder, venture_folder, wiki_folder, model, augmented_idf, stem, sanitize, lines, keep_ui):\n",
    "    '''get the similarity matric using embeddings'''\n",
    "    judges, ventures = get_parsed_data(judge_folder, venture_folder, stem=True, sanitize=True, lines=False, keep_ui=False)\n",
    "    judge_to_ind = {judge:ind for ind, judge in enumerate(judges.keys())}\n",
    "    venture_to_ind = {venture:ind for ind, venture in enumerate(ventures.keys())}\n",
    "\n",
    "    if augmented_idf:\n",
    "        idf, _ = get_aug_tfidf([judges, ventures], wiki_folder, stem, sanitize, lines=False, keep_ui=keep_ui)\n",
    "    else:\n",
    "        idf = get_smoothed_tfidf(judge_folder, venture_folder, wiki_folder, stem, sanitize, lines=True, keep_ui=keep_ui)\n",
    "\n",
    "    judge_line, venture_line = get_parsed_data(judge_folder, venture_folder, stem, sanitize, lines, keep_ui)\n",
    "    judge_rows = [(judge_to_ind[judge], judge_line[judge]) for judge in judge_to_ind]\n",
    "    judge_df = pd.DataFrame(judge_rows, columns=[\"index\", \"info\"])\n",
    "    venture_rows = [(venture_to_ind[venture], venture_line[venture]) for venture in venture_to_ind]\n",
    "    venture_df = pd.DataFrame(venture_rows, columns=[\"index\", \"info\"])\n",
    "\n",
    "    tokenizer, pretrained_model = get_tokenizer_and_model(model)\n",
    "\n",
    "    judge_embeddings, judge_idf_weights, judge_mask = tokenize_hybrid(\n",
    "        judge_df, model, tokenizer, pretrained_model, idf)\n",
    "    \n",
    "    venture_embeddings, venture_idf_weights, venture_mask= tokenize_hybrid(\n",
    "        venture_df, model, tokenizer, pretrained_model, idf)\n",
    "\n",
    "    return judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, judge_to_ind, venture_to_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e19a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid tokenization took 1.1 seconds\n",
      "Hybrid tokenization took 0.43 seconds\n",
      "Calculating similarities took: 0.01 seconds\n",
      "Calculating similarities took: 0.09 seconds\n",
      "Calculating similarities took: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# get the embeddings, weights, masks, and mappings for the judge and ventures\n",
    "judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, judge_to_ind, venture_to_ind = get_hybrid_embeddings(\n",
    "                                                                            judge_folder=judge_folder, \n",
    "                                                                            venture_folder=venture_folder, \n",
    "                                                                            wiki_folder=None, # input None or the wikipedia folder to supplement the idf\n",
    "                                                                            model='bert-base-uncased', # the name of the model used \n",
    "                                                                            augmented_idf=False, # whether to use the augmented idf or use the vanilla idf\n",
    "                                                                            stem=True, # whether we stem when preprocessing the data\n",
    "                                                                            sanitize=True, # whether we sanitize when preprocessing the data\n",
    "                                                                            lines=True, # whether we want to input a str passage (lines=True) or a list of str words (lines=False)\n",
    "                                                                            keep_ui=True) # whether to keep the uninformative words or not\n",
    "\n",
    "# find the similarity matrix\n",
    "# toke_level is a bool to indicate whether to use token level similarity or not\n",
    "# linear_kernel is a bool to indicate whether to use normalization (linear_kernel=False) or not (linear_kernel=True)\n",
    "similarity_matrix_token_kernel = get_hybrid_sim(judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, token_level=True, linear_kernel=True)\n",
    "similarity_matrix_token = get_hybrid_sim(judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, token_level=True, linear_kernel=False)\n",
    "similarity_matrix = get_hybrid_sim(judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, token_level=False, linear_kernel=False)\n",
    "# save the similarity matrix\n",
    "np.savetxt(f\"{folder_name}/Output/hybrid_similarities.txt\", similarity_matrix)\n",
    "np.savetxt(f\"{folder_name}/Output/hybrid_token_similarities.txt\", similarity_matrix_token)\n",
    "np.savetxt(f\"{folder_name}/Output/hybrid_token_kernel_similarities.txt\", similarity_matrix_token_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(similarity_matrix_token_kernel, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(similarity_matrix_token, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(similarity_matrix, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3bf14",
   "metadata": {},
   "source": [
    "# 2. Get Evaluations\n",
    "## Example Use Case\n",
    "here we randomly generate a manual score dic and perform the evaluations. For actual evaluations, the manual score dic contains score that are manually curated by expert humans on the match quality of judge venture pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef039bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "def get_random_manual_scores(folder_name):\n",
    "    '''generate random manual scores for judge venture pairs'''\n",
    "    manual_scores = {}\n",
    "    judge_folder = f\"{folder_name}/Input/Judge_Folder\"\n",
    "    venture_folder = f\"{folder_name}/Input/Venture_Folder\"\n",
    "\n",
    "    j_folder_path = Path(judge_folder)\n",
    "    v_folder_path = Path(venture_folder)\n",
    "    n_judges = len([i for i in j_folder_path.glob(\"*.txt\")])\n",
    "    n_ventures = len([i for i in v_folder_path.glob(\"*.txt\")])\n",
    "    for i in range(n_judges):\n",
    "        for j in range(n_ventures):\n",
    "            manual_scores[(i, j)] = np.random.randint(1,5)\n",
    "    return manual_scores\n",
    "\n",
    "manual_scores = get_random_manual_scores('example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e00c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking based on manually labeled scores vs similarity-based buckets\n",
      "0.09, p=0.517\n",
      "\n",
      "Manual scores vs full similarity rank order\n",
      "0.06, p=0.650\n",
      "\n",
      "Pair: (0, 0)\n",
      "Manual: 4, Bucket Rank: 4.0, Full Rank: 4.0, Rank Index: 33, Sim Value: 0.8998345136642456\n",
      "\n",
      "Pair: (0, 1)\n",
      "Manual: 4, Bucket Rank: 3.0, Full Rank: 3.0, Rank Index: 22, Sim Value: 0.8861034512519836\n",
      "\n",
      "Pair: (0, 2)\n",
      "Manual: 2, Bucket Rank: 2.0, Full Rank: 2.0, Rank Index: 7, Sim Value: 0.8570132851600647\n",
      "\n",
      "Pair: (0, 3)\n",
      "Manual: 2, Bucket Rank: 1.0, Full Rank: 2.0, Rank Index: 6, Sim Value: 0.8565527200698853\n",
      "\n",
      "Pair: (0, 4)\n",
      "Manual: 2, Bucket Rank: 4.0, Full Rank: 4.0, Rank Index: 32, Sim Value: 0.8997251391410828\n",
      "\n",
      "Pair: (0, 5)\n",
      "Manual: 4, Bucket Rank: 1.0, Full Rank: 2.0, Rank Index: 4, Sim Value: 0.8552368879318237\n",
      "\n",
      "Pair: (1, 0)\n",
      "Manual: 3, Bucket Rank: 4.0, Full Rank: 3.0, Rank Index: 28, Sim Value: 0.8946714401245117\n",
      "\n",
      "Pair: (1, 1)\n",
      "Manual: 4, Bucket Rank: 4.0, Full Rank: 4.0, Rank Index: 34, Sim Value: 0.9126430153846741\n",
      "\n",
      "Pair: (1, 2)\n",
      "Manual: 1, Bucket Rank: 3.0, Full Rank: 4.0, Rank Index: 24, Sim Value: 0.887894868850708\n",
      "\n",
      "Pair: (1, 3)\n",
      "Manual: 4, Bucket Rank: 3.0, Full Rank: 3.0, Rank Index: 21, Sim Value: 0.886017918586731\n",
      "\n",
      "Pair: (1, 4)\n",
      "Manual: 4, Bucket Rank: 2.0, Full Rank: 3.0, Rank Index: 11, Sim Value: 0.8692020177841187\n",
      "\n",
      "Pair: (1, 5)\n",
      "Manual: 2, Bucket Rank: 3.0, Full Rank: 4.0, Rank Index: 18, Sim Value: 0.8826945424079895\n",
      "\n",
      "Pair: (2, 0)\n",
      "Manual: 1, Bucket Rank: 2.0, Full Rank: 1.0, Rank Index: 12, Sim Value: 0.8718893527984619\n",
      "\n",
      "Pair: (2, 1)\n",
      "Manual: 3, Bucket Rank: 1.0, Full Rank: 1.0, Rank Index: 1, Sim Value: 0.8368973731994629\n",
      "\n",
      "Pair: (2, 2)\n",
      "Manual: 4, Bucket Rank: 1.0, Full Rank: 1.0, Rank Index: 2, Sim Value: 0.8542172312736511\n",
      "\n",
      "Pair: (2, 3)\n",
      "Manual: 3, Bucket Rank: 1.0, Full Rank: 1.0, Rank Index: 3, Sim Value: 0.8548147082328796\n",
      "\n",
      "Pair: (2, 4)\n",
      "Manual: 1, Bucket Rank: 2.0, Full Rank: 3.0, Rank Index: 13, Sim Value: 0.8721297979354858\n",
      "\n",
      "Pair: (2, 5)\n",
      "Manual: 3, Bucket Rank: 1.0, Full Rank: 1.0, Rank Index: 0, Sim Value: 0.8354558944702148\n",
      "\n",
      "Pair: (3, 0)\n",
      "Manual: 4, Bucket Rank: 3.0, Full Rank: 3.0, Rank Index: 15, Sim Value: 0.8748105764389038\n",
      "\n",
      "Pair: (3, 1)\n",
      "Manual: 2, Bucket Rank: 3.0, Full Rank: 2.0, Rank Index: 19, Sim Value: 0.8831814527511597\n",
      "\n",
      "Pair: (3, 2)\n",
      "Manual: 3, Bucket Rank: 3.0, Full Rank: 4.0, Rank Index: 23, Sim Value: 0.8865848779678345\n",
      "\n",
      "Pair: (3, 3)\n",
      "Manual: 1, Bucket Rank: 4.0, Full Rank: 4.0, Rank Index: 26, Sim Value: 0.8897377848625183\n",
      "\n",
      "Pair: (3, 4)\n",
      "Manual: 3, Bucket Rank: 2.0, Full Rank: 1.0, Rank Index: 8, Sim Value: 0.8660851120948792\n",
      "\n",
      "Pair: (3, 5)\n",
      "Manual: 3, Bucket Rank: 3.0, Full Rank: 3.0, Rank Index: 16, Sim Value: 0.8781230449676514\n",
      "\n",
      "Pair: (4, 0)\n",
      "Manual: 3, Bucket Rank: 3.0, Full Rank: 2.0, Rank Index: 14, Sim Value: 0.8738335371017456\n",
      "\n",
      "Pair: (4, 1)\n",
      "Manual: 4, Bucket Rank: 4.0, Full Rank: 3.0, Rank Index: 27, Sim Value: 0.8916087746620178\n",
      "\n",
      "Pair: (4, 2)\n",
      "Manual: 3, Bucket Rank: 3.0, Full Rank: 3.0, Rank Index: 20, Sim Value: 0.8854525089263916\n",
      "\n",
      "Pair: (4, 3)\n",
      "Manual: 1, Bucket Rank: 2.0, Full Rank: 3.0, Rank Index: 10, Sim Value: 0.8687740564346313\n",
      "\n",
      "Pair: (4, 4)\n",
      "Manual: 2, Bucket Rank: 2.0, Full Rank: 2.0, Rank Index: 9, Sim Value: 0.8677191734313965\n",
      "\n",
      "Pair: (4, 5)\n",
      "Manual: 1, Bucket Rank: 1.0, Full Rank: 3.0, Rank Index: 5, Sim Value: 0.8565506935119629\n",
      "\n",
      "Pair: (5, 0)\n",
      "Manual: 2, Bucket Rank: 4.0, Full Rank: 4.0, Rank Index: 29, Sim Value: 0.8950369358062744\n",
      "\n",
      "Pair: (5, 1)\n",
      "Manual: 3, Bucket Rank: 4.0, Full Rank: 4.0, Rank Index: 35, Sim Value: 0.9153174161911011\n",
      "\n",
      "Pair: (5, 2)\n",
      "Manual: 4, Bucket Rank: 3.0, Full Rank: 3.0, Rank Index: 17, Sim Value: 0.8826559782028198\n",
      "\n",
      "Pair: (5, 3)\n",
      "Manual: 3, Bucket Rank: 4.0, Full Rank: 4.0, Rank Index: 25, Sim Value: 0.8883862495422363\n",
      "\n",
      "Pair: (5, 4)\n",
      "Manual: 1, Bucket Rank: 4.0, Full Rank: 4.0, Rank Index: 31, Sim Value: 0.898994505405426\n",
      "\n",
      "Pair: (5, 5)\n",
      "Manual: 4, Bucket Rank: 4.0, Full Rank: 4.0, Rank Index: 30, Sim Value: 0.8976130485534668\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from evaluate import get_percentile, get_scoremat_from_sim, get_ranking\n",
    "\n",
    "# load the similarity matrix\n",
    "sim_mat = np.loadtxt(\"example/Output/embed_similarities.txt\")\n",
    "# define output path\n",
    "output_path = \"example/Output/embed_kendall_tau.txt\"\n",
    "# get the percentile distribution of scores of the manual score\n",
    "percentiles = get_percentile(manual_scores)\n",
    "# use the similarity matrix and transform it into a score matrix with values of 1 - 5 instead of simliarity scores\n",
    "scoremat_from_sim = get_scoremat_from_sim(sim_mat, percentiles)\n",
    "# get the kendall tau-b rank coefficients between the manual scores and algorithmic scores\n",
    "score_ranks = get_ranking(sim_mat, manual_scores, percentiles, scoremat_from_sim, output_path)\n",
    "with open(output_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428e72d",
   "metadata": {},
   "source": [
    "# 3. Ensemble Learning Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation 0\n",
      "Weights: [1.06058716e-15 5.34692103e-01 4.65307897e-01]\n",
      "Average Train Difference: 1.0714285714285714\n",
      "Average Test Difference: 1.0\n",
      "\n",
      "Cross-validation 1\n",
      "Weights: [1.59904488e-14 5.26941188e-01 4.73058812e-01]\n",
      "Average Train Difference: 0.8275862068965517\n",
      "Average Test Difference: 0.2857142857142857\n",
      "\n",
      "Cross-validation 2\n",
      "Weights: [2.10950008e-17 5.55907686e-01 4.44092314e-01]\n",
      "Average Train Difference: 1.3103448275862069\n",
      "Average Test Difference: 0.0\n",
      "\n",
      "Cross-validation 3\n",
      "Weights: [5.88258238e-15 5.38859245e-01 4.61140755e-01]\n",
      "Average Train Difference: 1.1724137931034482\n",
      "Average Test Difference: 1.4285714285714286\n",
      "\n",
      "Cross-validation 4\n",
      "Weights: [0.         0.54159909 0.45840091]\n",
      "Average Train Difference: 1.103448275862069\n",
      "Average Test Difference: 1.1428571428571428\n",
      "\n",
      "Final Avg Weight Diff: 1.1666666666666667\n"
     ]
    }
   ],
   "source": [
    "from ensemble import optimize_similarity, get_score_difference\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "folder_name = 'example'\n",
    "filenames = ['example/Output/embed_similarities.txt', \n",
    "             'example/Output/hybrid_similarities.txt', \n",
    "             'example/Output/tfidf_similarities.txt']\n",
    "# get a random set of manual scores\n",
    "manual_scores = get_random_manual_scores(folder_name)\n",
    "# define k fold \n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "avg = []\n",
    "\n",
    "score_keys = list(manual_scores.keys())\n",
    "score_vals = list(manual_scores.values())\n",
    "all_weights = []\n",
    "# iterate through each fold\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(score_keys)):\n",
    "    print(\"\\nCross-validation\", i)\n",
    "    # get the train data and test data \n",
    "    train_score_dic = {\n",
    "        score_keys[j]: score_vals[j] for j in train_index\n",
    "    }\n",
    "    test_score_dic = {\n",
    "        score_keys[j]: score_vals[j] for j in test_index\n",
    "    }\n",
    "\n",
    "    # use MLE to optimize over the algorithmic methods\n",
    "    weights, X_train, y_train, train_mean_diff = optimize_similarity(filenames, train_score_dic)\n",
    "    # get the test loss\n",
    "    test_mean_diff = get_score_difference(filenames, test_score_dic, weights)\n",
    "    all_weights.append(weights)\n",
    "    print(\"Average Train Difference:\", train_mean_diff)\n",
    "    print('Average Test Difference:', test_mean_diff)\n",
    "\n",
    "# get the average of weights from the 5 fold \n",
    "all_weights = np.array(all_weights)\n",
    "avg_weights = np.mean(all_weights, axis=0)\n",
    "diff = get_score_difference(filenames, manual_scores, avg_weights)\n",
    "print(\"\\nFinal Avg Weight Diff:\", diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
