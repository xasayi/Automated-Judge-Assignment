{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbfbde4",
   "metadata": {},
   "source": [
    "#  1. Get similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d64cf",
   "metadata": {},
   "source": [
    "run the following cell to specify what data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "folder_name = \"example\"\n",
    "judge_folder = f\"{folder_name}/Input/Judge_Folder\"\n",
    "venture_folder = f\"{folder_name}/Input/Venture_Folder\"\n",
    "\n",
    "os.makedirs(f\"{folder_name}/Output\", exist_ok=True)\n",
    "\n",
    "def print_max_pair(matrix, judge_to_ind, venture_to_ind):\n",
    "    judge_i, venture_i = np.unravel_index(np.argmax(matrix), matrix.shape)\n",
    "    max_sim_score = np.max(matrix)\n",
    "    ind_to_judge = {ind:judge for judge, ind in judge_to_ind.items()}\n",
    "    ind_to_venture = {ind:venture for venture, ind in venture_to_ind.items()}\n",
    "    print('Similarity score:', max_sim_score)\n",
    "    with open(f'{folder_name}/Input/Judge_Folder/{ind_to_judge[judge_i]}.txt', 'r') as f:\n",
    "        print(f'Judge {ind_to_judge[judge_i]}.txt, {judge_i}')\n",
    "        print(f.read())\n",
    "    with open(f'{folder_name}/Input/Venture_Folder/{ind_to_venture[venture_i]}.txt', 'r') as f:\n",
    "        print(f'Venture {ind_to_venture[venture_i]}.txt, {venture_i}')\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e4c34",
   "metadata": {},
   "source": [
    "## Using TFIDF\n",
    "### 1. Use augmented TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40aa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.tfidf_sim import get_aug_tfidf, tfidf_sim_aug\n",
    "from preprocess_data import get_parsed_data\n",
    "\n",
    "def get_tfidf_sim_aug(judge_folder, venture_folder, wiki_folder, stem, sanitize, lines, keep_ui):\n",
    "    judges, ventures = get_parsed_data(judge_folder, venture_folder, stem, sanitize, lines, keep_ui)\n",
    "    judge_to_ind = {judge:ind for ind, judge in enumerate(judges.keys())}\n",
    "    venture_to_ind = {venture:ind for ind, venture in enumerate(ventures.keys())}\n",
    "    idf, _, = get_aug_tfidf([judges, ventures], wiki_folder, stem, sanitize, lines, keep_ui)\n",
    "    similarity_matrix = -np.ones((len(judges), len(ventures)))\n",
    "\n",
    "    for judge, j_ind in judge_to_ind.items():\n",
    "        for venture, v_ind in venture_to_ind.items():\n",
    "            j_counter, v_counter = judges[judge], ventures[str(venture)]\n",
    "\n",
    "            if len(j_counter) != 0 and len(v_counter) != 0:\n",
    "                similarity_matrix[j_ind, v_ind] = tfidf_sim_aug(\n",
    "                    j_counter, v_counter, idf\n",
    "                )\n",
    "    return similarity_matrix, judge_to_ind, venture_to_ind\n",
    "\n",
    "similarity_matrix_tfidf_aug, judge_to_ind, venture_to_ind = get_tfidf_sim_aug(judge_folder=judge_folder, \n",
    "                                                                venture_folder=venture_folder, \n",
    "                                                                wiki_folder=None, \n",
    "                                                                stem=True, \n",
    "                                                                sanitize=True, \n",
    "                                                                lines=False, \n",
    "                                                                keep_ui=False)\n",
    "np.savetxt(f\"{folder_name}/Output/tfidf_aug_similarities.txt\", similarity_matrix_tfidf_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eceae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(similarity_matrix_tfidf_aug, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1a541",
   "metadata": {},
   "source": [
    "### 2. Use vanilla TFIDF with IDF smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77814018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from preprocess_data import parse_info, get_parsed_data\n",
    "\n",
    "wiki_folder = 'wikipedia_files'\n",
    "stem = True\n",
    "sanitize = True\n",
    "lines = True\n",
    "keep_ui = False\n",
    "\n",
    "def get_tfidf_sim(judge_folder, venture_folder, wiki_folder, stem, sanitize, lines, keep_ui):\n",
    "    count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    tfidf = TfidfTransformer(\n",
    "        smooth_idf=True, use_idf=True, norm=\"l2\", sublinear_tf=False\n",
    "    )\n",
    "\n",
    "    judges, ventures = get_parsed_data(judge_folder, venture_folder, stem, sanitize, lines, keep_ui)\n",
    "    judge_to_ind = {judge:ind for ind, judge in enumerate(judges.keys())}\n",
    "    venture_to_ind = {venture:ind for ind, venture in enumerate(ventures.keys())}\n",
    "\n",
    "    judge_values = list(judges.values())\n",
    "    venture_values = list(ventures.values())\n",
    "\n",
    "    if wiki_folder is not None:\n",
    "        wiki_dic = parse_info(wiki_folder, stem, sanitize, lines, keep_ui)\n",
    "        wiki_values = list(wiki_dic.values())\n",
    "        word_count_vec = count_vectorizer.fit_transform(\n",
    "            judge_values + venture_values + wiki_values\n",
    "        )\n",
    "    else:\n",
    "        word_count_vec = count_vectorizer.fit_transform(judge_values + venture_values)\n",
    "\n",
    "    tfidf.fit(word_count_vec)\n",
    "    count_vec = count_vectorizer.transform(judge_values + venture_values)\n",
    "    tfidf_vec = tfidf.transform(count_vec)\n",
    "    similarity_matrix = linear_kernel(tfidf_vec, tfidf_vec)[\n",
    "        : len(judge_values), len(judge_values) :\n",
    "    ]\n",
    "    return similarity_matrix, judge_to_ind, venture_to_ind \n",
    "\n",
    "similarity_matrix_tfidf, judge_to_ind, venture_to_ind = get_tfidf_sim(judge_folder=judge_folder, \n",
    "                                                                venture_folder=venture_folder, \n",
    "                                                                wiki_folder=None, \n",
    "                                                                stem=True, \n",
    "                                                                sanitize=True, \n",
    "                                                                lines=True, \n",
    "                                                                keep_ui=False)\n",
    "np.savetxt(f\"{folder_name}/Output/tfidf_similarities.txt\", similarity_matrix_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(similarity_matrix_tfidf, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5497af",
   "metadata": {},
   "source": [
    "## Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eeb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from similarity.embed_sim import get_tokenizer_and_model, tokenize, token_similarity\n",
    "from preprocess_data import get_parsed_data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def get_embedding_sim(judge_folder, venture_folder, model, stem, sanitize, lines, keep_ui, token_level):\n",
    "    judges, ventures = get_parsed_data(judge_folder, venture_folder, stem, sanitize, lines, keep_ui)\n",
    "    judge_to_ind = {judge:ind for ind, judge in enumerate(judges.keys())}\n",
    "    venture_to_ind = {venture:ind for ind, venture in enumerate(ventures.keys())}\n",
    "\n",
    "    judge_rows = [(judge_to_ind[judge], judges[judge]) for judge in judge_to_ind]\n",
    "    judge_df = pd.DataFrame(judge_rows, columns=[\"index\", \"info\"])\n",
    "    venture_rows = [(venture_to_ind[venture], ventures[venture]) for venture in venture_to_ind]\n",
    "    venture_df = pd.DataFrame(venture_rows, columns=[\"index\", \"info\"])\n",
    "\n",
    "    tokenizer, pretrained_model = get_tokenizer_and_model(model)\n",
    "    judge_embeddings, mean_judge_embed, judge_mask = tokenize(judge_df, model, tokenizer, pretrained_model)\n",
    "    venture_embeddings, mean_venture_embed, venture_mask = tokenize(venture_df, model, tokenizer, pretrained_model)\n",
    "    start = time.time()\n",
    "    if not token_level:\n",
    "        similarity_matrix = cosine_similarity(mean_judge_embed.numpy(), mean_venture_embed.numpy())\n",
    "    else:\n",
    "        similarity_matrix = token_similarity(judge_embeddings, judge_mask, venture_embeddings, venture_mask)\n",
    "    end = time.time()\n",
    "    print(f'Calculating similarity matrix took: {round(end-start, 2)} seconds')\n",
    "    return similarity_matrix, judge_to_ind, venture_to_ind\n",
    "\n",
    "similarity_matrix_embed_token, judge_to_ind, venture_to_ind = get_embedding_sim(judge_folder=judge_folder, \n",
    "                                                                venture_folder=venture_folder, \n",
    "                                                                model='bert-base-uncased',\n",
    "                                                                stem=True, \n",
    "                                                                sanitize=True, \n",
    "                                                                lines=True, \n",
    "                                                                keep_ui=True,\n",
    "                                                                token_level=True)\n",
    "np.savetxt(f\"{folder_name}/Output/embed_token_similarities.txt\", similarity_matrix_embed_token)\n",
    "similarity_matrix_embed, judge_to_ind, venture_to_ind = get_embedding_sim(judge_folder=judge_folder, \n",
    "                                                                venture_folder=venture_folder, \n",
    "                                                                model='bert-base-uncased',\n",
    "                                                                stem=True, \n",
    "                                                                sanitize=True, \n",
    "                                                                lines=True, \n",
    "                                                                keep_ui=True,\n",
    "                                                                token_level=False)\n",
    "np.savetxt(f\"{folder_name}/Output/embed_similarities.txt\", similarity_matrix_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(np.array(similarity_matrix_embed), judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(np.array(similarity_matrix_embed_token), judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0c20e",
   "metadata": {},
   "source": [
    "## Using Hybrid Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from similarity.tfidf_sim import get_aug_tfidf, get_smoothed_tfidf\n",
    "from similarity.hybrid_sim import tokenize_hybrid, get_hybrid_sim\n",
    "from similarity.embed_sim import get_tokenizer_and_model\n",
    "from preprocess_data import get_parsed_data\n",
    "\n",
    "\n",
    "def get_hybrid_embeddings(judge_folder, venture_folder, wiki_folder, model, augmented_idf, stem, sanitize, lines, keep_ui):\n",
    "    judges, ventures = get_parsed_data(judge_folder, venture_folder, stem=True, sanitize=True, lines=False, keep_ui=False)\n",
    "    judge_to_ind = {judge:ind for ind, judge in enumerate(judges.keys())}\n",
    "    venture_to_ind = {venture:ind for ind, venture in enumerate(ventures.keys())}\n",
    "\n",
    "    if augmented_idf:\n",
    "        idf, _ = get_aug_tfidf([judges, ventures], wiki_folder, stem, sanitize, lines=False, keep_ui=keep_ui)\n",
    "    else:\n",
    "        idf = get_smoothed_tfidf(judge_folder, venture_folder, wiki_folder, stem, sanitize, lines=True, keep_ui=keep_ui)\n",
    "\n",
    "    judge_line, venture_line = get_parsed_data(judge_folder, venture_folder, stem, sanitize, lines, keep_ui)\n",
    "    judge_rows = [(judge_to_ind[judge], judge_line[judge]) for judge in judge_to_ind]\n",
    "    judge_df = pd.DataFrame(judge_rows, columns=[\"index\", \"info\"])\n",
    "    venture_rows = [(venture_to_ind[venture], venture_line[venture]) for venture in venture_to_ind]\n",
    "    venture_df = pd.DataFrame(venture_rows, columns=[\"index\", \"info\"])\n",
    "\n",
    "    tokenizer, pretrained_model = get_tokenizer_and_model(model)\n",
    "\n",
    "    judge_embeddings, judge_idf_weights, judge_mask = tokenize_hybrid(\n",
    "        judge_df, model, tokenizer, pretrained_model, idf)\n",
    "    \n",
    "    venture_embeddings, venture_idf_weights, venture_mask= tokenize_hybrid(\n",
    "        venture_df, model, tokenizer, pretrained_model, idf)\n",
    "\n",
    "    return judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, judge_to_ind, venture_to_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, judge_to_ind, venture_to_ind = get_hybrid_embeddings(\n",
    "                                                                            judge_folder=judge_folder, \n",
    "                                                                            venture_folder=venture_folder, \n",
    "                                                                            wiki_folder=None, \n",
    "                                                                            model='bert-base-uncased', \n",
    "                                                                            augmented_idf=False, \n",
    "                                                                            stem=True, \n",
    "                                                                            sanitize=True, \n",
    "                                                                            lines=True, \n",
    "                                                                            keep_ui=True)\n",
    "            \n",
    "similarity_matrix_token_kernel = get_hybrid_sim(judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, token_level=True, linear_kernel=True)\n",
    "similarity_matrix_token = get_hybrid_sim(judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, token_level=True, linear_kernel=False)\n",
    "similarity_matrix = get_hybrid_sim(judge_embeddings, judge_idf_weights, judge_mask, venture_embeddings, venture_idf_weights, venture_mask, token_level=False, linear_kernel=False)\n",
    "\n",
    "np.savetxt(f\"{folder_name}/Output/hybrid_similarities.txt\", similarity_matrix)\n",
    "np.savetxt(f\"{folder_name}/Output/hybrid_token_similarities.txt\", similarity_matrix_token)\n",
    "np.savetxt(f\"{folder_name}/Output/hybrid_token_kernel_similarities.txt\", similarity_matrix_token_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(similarity_matrix_token_kernel, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(similarity_matrix_token, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_pair(similarity_matrix, judge_to_ind, venture_to_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3bf14",
   "metadata": {},
   "source": [
    "# 2. Get Evaluations\n",
    "## Example Use Case\n",
    "here we randomly generate a manual score dic and perform the evaluations. For actual evaluations, the manual score dic contains score that are manually curated by expert humans on the match quality of judge venture pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef039bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "def get_random_manual_scores(folder_name):\n",
    "    manual_scores = {}\n",
    "    judge_folder = f\"{folder_name}/Input/Judge_Folder\"\n",
    "    venture_folder = f\"{folder_name}/Input/Venture_Folder\"\n",
    "\n",
    "    j_folder_path = Path(judge_folder)\n",
    "    v_folder_path = Path(venture_folder)\n",
    "    n_judges = len([i for i in j_folder_path.glob(\"*.txt\")])\n",
    "    n_ventures = len([i for i in v_folder_path.glob(\"*.txt\")])\n",
    "    for i in range(n_judges):\n",
    "        for j in range(n_ventures):\n",
    "            manual_scores[(i, j)] = np.random.randint(1,5)\n",
    "    return manual_scores\n",
    "\n",
    "manual_scores = get_random_manual_scores('example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import get_percentile, get_scoremat_from_sim, get_ranking\n",
    "\n",
    "sim_mat = np.loadtxt(\"example/Output/embed_similarities.txt\")\n",
    "output_path = \"example/Output/embed_kendall_tau.txt\"\n",
    "percentiles = get_percentile(manual_scores)\n",
    "scoremat_from_sim = get_scoremat_from_sim(sim_mat, percentiles)\n",
    "score_ranks = get_ranking(sim_mat, manual_scores, percentiles, scoremat_from_sim, output_path)\n",
    "with open(output_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428e72d",
   "metadata": {},
   "source": [
    "# 3. Ensemble Learning Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble import optimize_similarity, get_score_difference\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "folder_name = 'example'\n",
    "filenames = ['example/Output/embed_similarities.txt', \n",
    "             'example/Output/hybrid_similarities.txt', \n",
    "             'example/Output/tfidf_similarities.txt']\n",
    "manual_scores = get_random_manual_scores(folder_name)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "avg = []\n",
    "\n",
    "score_keys = list(manual_scores.keys())\n",
    "score_vals = list(manual_scores.values())\n",
    "all_weights = []\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(score_keys)):\n",
    "    print(\"\\nCross-validation\", i)\n",
    "    train_score_dic = {\n",
    "        score_keys[j]: score_vals[j] for j in train_index\n",
    "    }\n",
    "    test_score_dic = {\n",
    "        score_keys[j]: score_vals[j] for j in test_index\n",
    "    }\n",
    "\n",
    "    weights, X_train, y_train, train_mean_diff = optimize_similarity(filenames, train_score_dic)\n",
    "    test_mean_diff = get_score_difference(filenames, test_score_dic, weights)\n",
    "    all_weights.append(weights)\n",
    "    print(\"Average Train Difference:\", train_mean_diff)\n",
    "    print('Average Test Difference:', test_mean_diff)\n",
    "\n",
    "all_weights = np.array(all_weights)\n",
    "avg_weights = np.mean(all_weights, axis=0)\n",
    "diff = get_score_difference(filenames, manual_scores, avg_weights)\n",
    "print(\"\\nFinal Avg Weight Diff:\", diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
